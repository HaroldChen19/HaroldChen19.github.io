---
layout: default
---

<body>	
	
<div class="content heading anchor" id="home">
		<table style="margin-top: -4mm;">
		</table>
        <div style="margin-top: 0mm;" class="img">
			<img class="header-img" src="images/homepage3.jpg" alt="Photo" align="left" height="240" >
		</div>
        <div style="margin-top: 7mm;" class="header-text">
            <h2>Harold Haodong Chen</h2>
            <p>
				<!-- Incoming Ph.D. <br> 
				Hong Kong University of Science and Technology <br> -->
				Undergraduate <br>  
				Northwestern Polytechnical University <br>
				<!-- Academy of Interdisciplinary Studies <br>
				Hong Kong University of Science and Technology <br> -->
				Email: haroldchen328 [at] gmail.com <br>
            </p>
		<div id="contact">	
			<a href="https://scholar.google.com/citations?hl=en&user=BGUdPBAAAAAJ" class="icon">
          			<img src="images/ico/scholar.png"  alt="View Harold Chen's profile on Google Scholar", height="30px" style="margin-bottom:-3px">
          		</a>
          		<a href="https://github.com/HaroldChen19" class="icon">
          			<img src="images/ico/github_s.jpg"  alt="View Harold Chen's codes on Github", height="30px" style="margin-bottom:-3px">
          		</a>
          		<a href="https://x.com/HaroldChen68578" class="icon">
					<img src="images/ico/x.png"  alt="View Harold Chen's profile on X", height="30px" style="margin-bottom:-3px">
				</a>
				<a href="javascript:void(0);" class="icon" onmouseover="showWeChatQRCode()" onmouseout="hideWeChatQRCode()">
					<img src="images/ico/wechat.jpeg"  alt="View Harold Chen's profile on X", height="30px" style="margin-bottom:-3px">
				</a>
        </div>
		<div id="wechat-qr-code" style="display: none; position: absolute; z-index: 100;">
			<img src="images/wechat2.png" alt="WeChat QR Code" style="width: 200px; height: 200px;" />
		</div>
		<script>
            function showWeChatQRCode() {
              var qrCode = document.getElementById('wechat-qr-code');
              qrCode.style.display = 'block';
            }
            
            function hideWeChatQRCode() {
              var qrCode = document.getElementById('wechat-qr-code');
              qrCode.style.display = 'none';
            }
		</script>
	  </div>
</div>	   

<table hegiht="50"><td hegiht="50"></td></table>
<table hegiht="50"><td hegiht="50"></td></table>
<table hegiht="50"><td hegiht="50"></td></table>

	   
<div class="content anchor" id="bio">     
	<h3 class="content-section">Biography</h3>

		<!-- <p style="text-align: justify">	I'm a final-year undergraduate student at Northwestern Polytechnical University (NPU) and an incoming PhD student (2025 Fall) at the Hong Kong University of Science and Technology (HKUST). I'm fortunate to work closely with Prof. <a href="https://sites.google.com/site/sernam" target="_blank">Ser-Nam Lim</a>, Prof. <a href="https://leehomyc.github.io/" target="_blank">Harry Yang</a>, Prof. <a href="https://cqf.io/" target="_blank">Qifeng Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=amxDSLoAAAAJ&hl=en" target="_blank">Dian Shao</a>.
		<br> -->
		<!-- <p style="text-align: justify">	I'm a final-year undergraduate student at Northwestern Polytechnical University (NPU) and a research intern at <a href="https://www.everlyn.ai/research" target="_blank">Everlyn AI</a>. I'm fortunate to work closely with Prof. <a href="https://sites.google.com/site/sernam" target="_blank">Ser-Nam Lim</a>, Prof. <a href="https://leehomyc.github.io/" target="_blank">Harry Yang</a>, Prof. <a href="https://cqf.io/" target="_blank">Qifeng Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=amxDSLoAAAAJ&hl=en" target="_blank">Dian Shao</a>.
			<br> -->
		<p style="text-align: justify">	I'm a final-year undergraduate student at Northwestern Polytechnical University (NPU). I'm fortunate to work closely with Prof. <a href="https://sites.google.com/site/sernam" target="_blank">Ser-Nam Lim</a>, Prof. <a href="https://leehomyc.github.io/" target="_blank">Harry Yang</a>, Prof. <a href="https://cqf.io/" target="_blank">Qifeng Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=amxDSLoAAAAJ&hl=en" target="_blank">Dian Shao</a>.
			<br>

		<!-- <p style="text-align: justify"> Previously, I was advised by Prof. <a href="https://scholar.google.com/citations?user=amxDSLoAAAAJ&hl=en" target="_blank">Dian Shao</a> from both NPU and Shanghai AI Laboratory, which contributed to my development in this field. I also worked with Prof. <a href="https://scholar.google.com/citations?user=8P7q1wQAAAAJ&hl=zh-CN" target="_blank">Mulin Chen</a> and Prof. <a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN" target="_blank">Xuelong Li</a> from TeleAI, China Telecom, as well as Prof. <a href="https://scholar.google.com/citations?user=n9cODgcAAAAJ" target="_blank">Yuxuan Liang</a> from HKUST(GZ). -->
			
		<p style="text-align: justify"> I am always open to all forms of research collaboration. Feel free to contact me if you are interested in working with me! My research interests include:
		</p>
		<ul>
			<li><strong>Video Generation:</strong> RFT, test-time, efficient generation</li>
			<!-- <li><strong>Video Understanding:</strong> multi-modal, fine-grained</li> -->
			<li><strong>Large Video Model:</strong> RFT, DPO, unified understanding &amp; generation </li>
		</ul>
</div>   
	
<div class="content anchor" id="news">  
	<h3 class="content-section-list">News</h3>
	<p>
	<li>
		[05/2025] <a href="https://arxiv.org/abs/2504.13122" target="_blank">VistaDPO</a> is accepted to ICML'25!
	<li>
		[02/2025] <a href="https://arxiv.org/abs/2505.13437" target="_blank">FinePhys</a> is accepted to CVPR'25!
	<li>
		[02/2025] Served as a reviewer for ICCV'25.
	<li>
		[02/2025] Served as a reviewer for Pattern Recognition.
	<!-- <li>
		[01/2025] Served as a reviewer for ACM TOMM. -->
	<li>
		[12/2024] Served as a reviewer for ICML'25.
	<li>
		[12/2024] <a href="https://arxiv.org/abs/2501.01245" target="_blank">SeFAR</a> is accepted to AAAI'25!
	<li>
		[11/2024] Served as a reviewer for CVPR'25.
	<!-- <li>
		[10/2024] Served as a reviewer for AISTATS'25. -->
	<li>
		[08/2024] Served as a reviewer for ICLR'25.
	<li>
		[07/2024] <a href="https://haroldchen19.github.io/FineCLIPER-Page/" target="_blank">FineCLIPER</a> and <a href="https://export.arxiv.org/abs/2404.09640" target="_blank">CREST</a> are accepted to MM'24!
	
	<details><summary>More</summary>
	<li>
		[05/2024] Served as a reviewer for NeurIPS'24.
	<li>
		[02/2024] Served as a reviewer for ECCV'24.
	<li>
		[01/2024] Served as a reviewer for MM'24.
	<li>
		[01/2024] <a href="https://dl.acm.org/doi/10.1145/3589334.3645378" target="_blank">UrbanCLIP</a> is accepted to WWW'24!
	</li>
	</details>
	</p>
</div>   

<div class="content anchor" id="publication">     
	<h3  class="content-section">
	Experience
	</h3>

	<table bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody>
			<tr valign="baseline">
				<td width="160">
					<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class="">
						<img src="./images/institute/everlyn_1.png" width="160" border="0">
					</p>
				</td>
				<td valign="middle">
					<p>
						<span style="font-weight: bold;">Research Intern </span>| Everlyn AI<br>
						Time: 8/2024 - 5/2025. Mentor: Prof. <a href="https://sites.google.com/site/sernam" target="_blank">Ser-Nam Lim</a>
					</p>
				</td>
			</tr>
		</tbody>
	</table>
	
	<table bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody>
			<tr valign="baseline">
				<td width="160">
					<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class="">
						<img src="./images/institute/hkgai.png" width="160" border="0">
					</p>
				</td>
				<td valign="middle">
					<p>
						<span style="font-weight: bold;">Research Intern </span>| HKGAI<br>
						Time: 5/2024 - 8/2024. Mentor: Prof. <a href="https://whluo.github.io/" target="_blank">Wenhan Luo</a>
					</p>
				</td>
			</tr>
		</tbody>
	</table>
	
	<table bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody>
			<tr valign="baseline">
				<td width="160">
					<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class="">
						<img src="./images/institute/NWPU.png" width="160" border="0">
					</p>
				</td>
				<td valign="middle">
					<p>
						<span style="font-weight: bold;">Research Intern </span>| DianLab, NPU<br>
						Time: 6/2023 - 8/2024. Advisor: Prof. <a href="https://scholar.google.com/citations?user=amxDSLoAAAAJ&hl=en" target="_blank">Dian Shao</a>
					</p>
				</td>
			</tr>
		</tbody>
	</table>
	

</div>


<div class="content anchor" id="publication">     
	<h3  class="content-section">
	Selected Preprints
	</h3>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/fluxflow_new.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> Temporal Regularization Makes Your Video Generator Stronger </span><br>
			<span style="font-weight: bold;"><u>Harold Haodong Chen</u></span>, Haojian Huang, Xianfeng Wu, Yexin Liu, Yajing Bai, Wen-Jie Shu, Harry Yang<sup>&dagger;</sup>, Ser-Nam Lim<sup>&dagger;</sup>
			<br>
			arXiv, 2025 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2503.15417">[arXiv]</a></span>
			<span class="tag"><a href="https://haroldchen19.github.io/FluxFlow/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/lightgen.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> LightGen: Efficient Image Generation through Knowledge Distillation and Direct Preference Optimization </span><br>
			Xianfeng Wu*, Yajing Bai*, Haoze Zheng*, <span style="font-weight: bold;"><u>Harold Haodong Chen</u>* <font color="lightcoral">(co-first)</font></span>, Yexin Liu*, Zihao Wang, Xuran Ma, Wen-Jie Shu, Harry Yang<sup>&dagger;</sup>, Ser-Nam Lim<sup>&dagger;</sup><br>
			arXiv, 2025 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2503.08619">[arXiv]</a></span>
			<span class="tag"><a href="https://github.com/XianfengWu01/LightGen">[Code]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/ues.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> Beyond Generation: Unlocking Universal Editing via Self-Supervised Fine-Tuning </span><br>
			<span style="font-weight: bold;"><u>Harold Haodong Chen</u></span>, Harry Yang<sup>&dagger;</sup>, Ser-Nam Lim<sup>&dagger;</sup>
			<br>
			arXiv, 2024 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2412.02114">[arXiv]</a></span>
			<span class="tag"><a href="https://haroldchen19.github.io/UES-Page/">[Webpage]</a></span>
			<span class="tag"><a href="https://github.com/HaroldChen19/UES">[Code]</a></span>
			<span class="tag"><a href="https://huggingface.co/datasets/Harold328/OmniBench-99">[Benchmark]</a></span>
		</td>
	</tbody>
	</table>

	<!-- <table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/sram.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> Beyond Uncertainty: Evidential Deep Learning for Robust Video Temporal Grounding
			</span><br>
			Kaijing Ma*, Haojian Huang*, Jin Chen*, <span style="font-weight: bold;"><u>Haodong Chen</u></span>, Pengliang Ji, Xianghao Zang, Han Fang, Chao Ban, Hao Sun, Mulin Chen<sup>&dagger;</sup>, Xuelong Li<sup>&dagger;</sup>
			<br>
			arXiv, 2024 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2408.16272">[arXiv]</a></span>
			<span class="tag"><a href="https://kaijing.space/SRAM/">[Webpage]</a></span>
			<span class="tag"><a href="https://github.com/KaijingOfficial/sram_vtg">[Code]</a></span>
		</td>
	</tbody>
	</table> -->

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/gsvton_new.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> GaussianVTON: 3D Human Virtual Try-ON via Multi-Stage Gaussian Splatting Editing with Image Prompting
			</span><br>
			<span style="font-weight: bold;"><u>Haodong Chen</u></span>, Yongle Huang, Haojian Huang, Xiangsheng Ge, Dian Shao<sup>&dagger;</sup>
			<br>
			arXiv, 2024 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2405.0747">[arXiv]</a></span>
			<span class="tag"><a href="https://haroldchen19.github.io/gsvton/">[Webpage]</a></span>
			<span class="tag"><a href="https://github.com/HaroldChen19/GaussianVTON">[Code]</a></span>
		</td>
	</tbody>
	</table>
	

	
</div>

	
<div class="content anchor" id="publication">     
	<h3  class="content-section">
	Publications
	</h3>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/vista.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> VistaDPO: Video Hierarchical Spatial-Temporal Direct Preference Optimization for Large Video Models </span><br>
			Haojian Huang*, <span style="font-weight: bold;"><u>Haodong Chen</u>* <font color="lightcoral">(co-first)</font></span>, Shengqiong Wu, Meng Luo, Jinlan Fu, Xinya Du, Hanwang Zhang, Hao Fei<sup>&dagger;</sup>
			<br>
			International Conference on Machine Learning
			(<span style="font-weight: bold;">ICML</span>), 2025 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2504.13122">[Paper]</a></span>
			<span class="tag"><a href="https://arxiv.org/abs/2504.13122">[arXiv]</a></span>
			<span class="tag"><a href="https://github.com/HaroldChen19/VistaDPO">[Code]</a></span>
			<span class="tag"><a href="https://huggingface.co/datasets/Harold328/VistaDPO-7K">[Dataset]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/finephys.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;">FinePhys: Fine-grained Human Action Generation by Explicitly Incorporating Physical Laws for Effective Skeletal Guidance</span><br>
			Dian Shao<sup>&dagger;</sup>, Mingfei Shi, Shengda Xu, <span style="font-weight: bold;"><u>Haodong Chen</u></span>, Yongle Huang, Binglu Wang
			<br>
			IEEE/CVF Computer Vision and Pattern Recognition
			(<span style="font-weight: bold;">CVPR</span>), 2025 <br>
			<span class="tag"><a href="https://arxiv.org/abs/2505.13437">[Paper]</a></span>
			<span class="tag"><a href="https://arxiv.org/abs/2505.13437">[arXiv]</a></span>
			<span class="tag"><a href="https://mingfeishims.github.io/projects-FinePhys/">[Webpage]</a></span>
			<span class="tag"><a href="https://github.com/SmartDianLab/FinePhys">[Code]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/sefar.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> SeFAR: Semi-supervised Fine-grained Action Recognition with Temporal Perturbation and Learning Stabilization
			</span><br>
			Yongle Huang*, <span style="font-weight: bold;"><u>Haodong Chen</u>* <font color="lightcoral">(co-first)</font></span>, Zhenbang Xu, Zihan Jia, Haozhou Sun, Dian Shao<sup>&dagger;</sup>
			<br>
			AAAI Conference on Artificial Intelligence
			(<span style="font-weight: bold;">AAAI</span>), 2025 <br>
			<span class="tag"><a href="https://ojs.aaai.org/index.php/AAAI/article/view/32400">[Paper]</a></span>
			<span class="tag"><a href="https://arxiv.org/abs/2501.01245">[arXiv]</a></span>
			<span class="tag"><a href="https://github.com/KyleHuang9/SeFAR">[Code]</a></span>
			<span class="tag"><a href="https://github.com/KyleHuang9/SeFAR">[Dataset]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/finecliper.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> FineCLIPER: Multi-modal Fine-grained CLIP for Dynamic Facial Expression Recognition with AdaptERs
			</span><br>
			<span style="font-weight: bold;"><u>Haodong Chen</u></span>, Haojian Huang, Junhao Dong, Mingzhe Zheng, Dian Shao<sup>&dagger;</sup> <br>
			ACM International Conference on Multimedia
			(<span style="font-weight: bold;">MM</span>), 2024 <br>
			<span class="tag"><a href="https://dl.acm.org/doi/10.1145/3664647.3680827">[Paper]</a></span>
			<span class="tag"><a href="https://arxiv.org/abs/2407.02157">[arXiv]</a></span>
			<span class="tag"><a href="https://haroldchen19.github.io/FineCLIPER-Page/">[Webpage]</a></span>
		</td>
	</tbody>
	</table>
	
	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/CREST.png" width="160"  border="0"></p>
		</td>
	<td  valign="middle">
		<p><span style="font-weight: bold;"> CREST: Cross-modal Resonance through Evidential Deep Learning for Enhanced Zero-Shot Learning </span><br>
			 Haojian Huang, Xiaozhen Qiao, Zhuo Chen, <span style="font-weight: bold;"><u>Haodong Chen</u></span>, Bingyu Li, Zhe Sun, Mulin Chen<sup>&dagger;</sup>, Xuelong Li<sup>&dagger;</sup>
			<br>
			ACM International Conference on Multimedia
			(<span style="font-weight: bold;">MM</span>), 2024 <br>
			<span class="tag"><a href="https://dl.acm.org/doi/10.1145/3664647.3681629">[Paper]</a></span>
			<span class="tag"><a href="https://arxiv.org/abs/2404.09640">[arXiv]</a></span>
			<span class="tag"><a href="https://github.com/JethroJames/CREST">[Code]</a></span>
		</td>
	</tbody>
	</table>

	<table  bordercolor="white" bordercolordark="white" bordercolorlight="white" cellpadding="0" cellspacing="0" height="45" bgcolor="white">
		<tbody><tr valign="baseline">
		<td width="160">
			<p align="center" style="margin-top:2.5mm; margin-right:1mm; margin-bottom:0; margin-left:0;" class=""><img src="./images/papers/urbanclip.png" width="160"  border="0"></p>
		</td>
		<td  valign="middle">
			<p><span style="font-weight: bold;"> UrbanCLIP: Learning Text-enhanced Urban Region Profiling with Contrastive Language-Image Pretraining from the Web
			</span><br>
			Yibo Yan, Haomin Wen, Siru Zhong, Wei Chen, <span style="font-weight: bold;"><u>Haodong Chen</u></span>, Qingsong Wen, Roger Zimmermann, Yuxuan Liang<sup>&dagger;</sup>
			<br>
			ACM International World Wide Web Conference
			(<span style="font-weight: bold;">WWW</span>), 2024 <span style="font-weight: bold;"><font color="#f00">(Oral Presentation)</font></span><br>
			<span class="tag"><a href="https://dl.acm.org/doi/10.1145/3589334.3645378">[Paper]</a></span>
			<span class="tag"><a href="https://arxiv.org/abs/2310.18340">[arXiv]</a></span>
			<span class="tag"><a href="https://www.youtube.com/watch?v=FdIZ7oJfmkg">[Video]</a></span>
			<span class="tag"><a href="https://github.com/StupidBuluchacha/UrbanCLIP">[Code]</a></span>
		</td>
	</tbody>
	</table>

	
</div>





<div class="content anchor" >    
	<h3 class="content-section-list">Awards and Honors</h3>  
		<p>
			<table style="border-spacing:2px" width="100%">
			<tbody>
			<tr><td><li>
				Outstanding University Student of NPU  </td> <td align="right"> 2024 </td></tr>
			<tr><td><li>
				Innovation and Entrepreneurship Advanced Individual Honor, NPU</td> <td align="right"> 2024 </td></tr>
			<tr><td><li>
				School Scholarship, NPU  </td> <td align="right"> 2024 </td></tr>
			<tr><td><li>
				University Student Innovation Fund, Ministry of Education of P.R. China </td> <td align="right"> 2023 </td></tr>
			<tr><td><li>
				Academic Advancement Individual Honor, NPU </td> <td align="right"> 2023 </td></tr>
			<tr><td><li>
				School Scholarship, NPU  </td> <td align="right"> 2023 </td></tr>
			</li>
			</tbody>
			</table>

			<!-- <details><summary>More</summary>
			<table style="border-spacing:2px" width="100%">
			<tbody>
			<tr><td><li>Nanjing University Outstanding Student Leader Award</td> <td align="right"> 2015</td></tr>
			</li>
			</tbody>
			</table>
			</details> -->
	</p>
</div>    

	
<!-- <div class="content anchor" id="talks">
	<h3 class="content-section-list">Talks</h3>

	<p>
		<table style="border-spacing:2px" width="100%">
		<body>

		<tr><td><li>Bridging the Representation Gap of Humans and Computers for Video Production</td></tr>
		<tr><td>
			<ul><ul>
				<li>China Golden Rooster Film Festival, 11/2024</li>
				<li>World Artificial Intelligence Conference (WAIC), 07/2024</li>
				<li>Shanghai Television Magnolia Festival, 06/2024</li>
				<li>University of Hong Kong (HKU), 05/2024</li>
				<li>Hong Kong University of Science and Technology (HKUST), 05/2024</li>
				<li>Stanford University, 04/2024</li>
				<li><a style="color:#333" href="https://datascience.hku.hk/hk-sh-ai-forum-2024/">Hong Kong Shanghai AI Forum</a>, 04/2024</li>
			</ul></ul>
		</td></tr>
		

		</tbody>
		</table>
	</p> -->

<!-- </div>    -->


<div class="content anchor" id="service">
	<h3 class="content-section-list">Services</h3>
	<p>
		<!-- <li>Conference Reviewer: Computer Vision and Pattern Recognition (CVPR, 2025), International Conference on Computer Vision (ICCV, 2025), European Conference on Computer Vision (ECCV, 2024), International Conference on Machine Learning (ICML, 2025), International Conference on Learning Representations (ICLR, 2025), Neural Information Processing Systems (NeurIPS, 2024-2025), ACM International Conference on Multimedia (MM, 2024), International Conference on Artificial Intelligence and Statistics (AISTATS, 2025)<br>
			 -->
		<li>Conference Reviewer,
			<br>&emsp;&emsp;Computer Vision and Pattern Recognition (CVPR), 2025
			<br>&emsp;&emsp;International Conference on Computer Vision (ICCV), 2025
			<br>&emsp;&emsp;European Conference on Computer Vision (ECCV), 2024
			<br>&emsp;&emsp;International Conference on Machine Learning (ICML), 2025
			<br>&emsp;&emsp;International Conference on Learning Representations (ICLR), 2025
			<br>&emsp;&emsp;Neural Information Processing Systems (NeurIPS), 2024-2025
			<br>&emsp;&emsp;ACM International Conference on Multimedia (MM), 2024
			<br>&emsp;&emsp;Artificial Intelligence and Statistics (AISTATS), 2025
		</li>
		<li>Journal Reviewer,
			<br>&emsp;&emsp;Pattern Recognition
			<br>&emsp;&emsp;ACM TOMM
		</li>
	</p>
</div>

	<div style="text-align:center; width:300px; height:300px; margin: 0 auto; overflow:hidden;">
		<script type='text/javascript' id='mapmyvisitors' 
				src='https://mapmyvisitors.com/map.js?cl=9d97b5&w=a&t=tt&d=K5mXqzOGNWeXE2Ezi93zbcP2GhxzuJjlVPOeC5nKM24&co=ffffff&cmo=9189eb&cmn=c77474&ct=2e2b2b'>
		</script>
	</div>

</body>
